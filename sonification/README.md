# UTAC Sonification: The Sound of Criticality ðŸŽµ

> *"What if you could hear emergence? What if critical transitions had a voice?"*

Transform UTAC threshold dynamics (Î², Î˜, R) into audio. Different field types produce distinct sonic signatures.

---

## ðŸŽ¼ Concept

The UTAC sonification maps threshold physics to acoustic properties:

| UTAC Parameter | Sonic Mapping | Why? |
|----------------|---------------|------|
| **Î² (steepness)** | Pitch/Frequency | Steeper transitions â†’ Higher pitch |
| **R-Î˜ (distance to threshold)** | Amplitude | Closer to threshold â†’ Louder |
| **Î˜ (threshold)** | Reference pitch | Critical point anchors the sound |
| **Î¶(R) (impedance)** | Filtering/Damping | Resonance vs. damping |
| **Field Type** | Timbre/Harmonics | Each field type has a unique "voice" |

---

## ðŸŽ¹ Field Type Acoustic Profiles

### 1. Strongly Coupled (Î²: 3.5-5.0)
- **Sound:** Warm, resonant, rich harmonics
- **Base:** A3 (220 Hz)
- **Examples:** Neural networks, AMOC, honeybees
- **Character:** Deep coupling produces sustained tones

### 2. High-Dimensional (Î²: 3.0-4.5)
- **Sound:** Complex, ethereal, floating
- **Base:** E4 (329.63 Hz)
- **Examples:** LLMs, evolutionary systems
- **Character:** High-dimensional phase space = complex overtones

### 3. Weakly Coupled (Î²: 2.0-3.5)
- **Sound:** Soft, diffuse, gentle
- **Base:** A2 (110 Hz)
- **Examples:** Neural plasticity, ecosystems
- **Character:** Weak coupling = few harmonics, slow transitions

### 4. Physically Constrained (Î²: 4.5-6.0+)
- **Sound:** Sharp, precise, percussive
- **Base:** A4 (440 Hz)
- **Examples:** Black holes, earthquakes
- **Character:** Physical constraints = clear fundamental

### 5. Meta-Adaptive (Î²: Variable, 2.5-16.3)
- **Sound:** Morphing, modulating, adaptive
- **Base:** C4 (261.63 Hz)
- **Examples:** Climate cascades, markets, urban heat
- **Character:** Variable Î² = constantly evolving timbre

---

## ðŸš€ Quick Start

### Installation

```bash
# Dependencies
pip install numpy scipy

# Optional: For analysis integration
pip install pandas
```

### Basic Usage

```bash
# Sonify a single transition
python -m sonification.utac_sonification \
  --beta 4.5 \
  --theta 100 \
  --output transition.wav

# Use a preset (LLM emergence)
python -m sonification.utac_sonification \
  --preset wei \
  --output llm_emergence.wav

# Sonic journey through field types
python -m sonification.utac_sonification \
  --preset field_types \
  --output field_spectrum.wav
```

### Python API

```python
from sonification import UTACsonifier

# Initialize
sonifier = UTACsonifier(sample_rate=44100, duration=3.0)

# Single transition
audio, metadata = sonifier.sonify_transition(
    beta=4.5,
    theta=100.0
)

# Multiple transitions (spectrum)
audio, metadata = sonifier.sonify_spectrum(
    beta_values=[2.5, 3.5, 4.5, 5.5],
    labels=["Weak", "High-dim", "Strong", "Physical"]
)

# Save
from sonification.utac_sonification import save_audio, save_metadata
save_audio(audio, "output.wav", sonifier.sample_rate)
save_metadata(metadata, "output.json")
```

---

## ðŸŽ¨ Presets

Located in `sonification/presets/`:

| Preset | Description | Î² | Field Type |
|--------|-------------|---|------------|
| `wei` | LLM emergence (GPT-3) | 3.47 | High-Dimensional |
| `amoc` | Ocean circulation collapse | 4.2 | Strongly Coupled |
| `urban_heat` | Extreme thermal transition | 16.3 | Meta-Adaptive |
| `honeybees` | Swarm collective decision | 4.0 | Strongly Coupled |
| `field_types` | Full field type spectrum | 2.5-8.0 | All Types |
| `criticality_journey` | Cross-domain narrative | 3.47-16.3 | Mixed |

---

## ðŸ§  Technical Details

### Sonification Algorithm

1. **Field Type Classification**
   - Classify Î² into one of five field types
   - Load corresponding acoustic profile (harmonics, envelope, timbre)

2. **Frequency Mapping**
   - Base frequency from field type profile
   - Î²-scaled multiplier: `freq = base_freq Ã— (1 + (Î² - 2) / 10)`
   - Higher Î² â†’ higher pitch

3. **Amplitude Modulation**
   - Compute Ïƒ(Î²(R-Î˜)) over time
   - Peak amplitude at threshold crossing (Ïƒ=0.5)
   - `amplitude = Ïƒ Ã— (1-Ïƒ) Ã— 4`

4. **Harmonic Synthesis**
   - Add harmonics from profile
   - Slight frequency modulation based on Ïƒ
   - Rich timbre from multiple overtones

5. **Envelope Shaping**
   - Apply field-type-specific envelope
   - Sustained, percussive, gentle, floating, or adaptive

### Audio Format

- **Sample Rate:** 44100 Hz (CD quality)
- **Bit Depth:** 16-bit signed integer
- **Channels:** Mono
- **Format:** WAV (requires scipy) or NumPy array

---

## ðŸŽ“ Educational Use Cases

### 1. Science Communication
- **Museums:** Interactive exhibits where visitors "play" with Î² sliders
- **Planetariums:** Sonify climate tipping points alongside visualizations
- **Galleries:** "The Sound of Criticality" art installation

### 2. Research
- **Pattern Detection:** Emergent patterns might be audible before visible
- **Outlier Analysis:** Extreme Î² values (like urban heat) sound *different*
- **Cross-Domain Comparison:** Hear similarities between LLMs and ecosystems

### 3. Teaching
- **Physics:** Teach phase transitions through sound
- **Math:** Logistic function Ïƒ(Î²(R-Î˜)) as sonic metaphor
- **Complexity Science:** Emergent properties across scales

---

## ðŸŽ­ Artistic Extensions

### Ideas for "The Sound of Criticality" Project

1. **Multi-Channel Installation**
   - 5 speakers, one per field type
   - Spatial audio representing Î²-space
   - Visitors walk through emergence landscape

2. **Interactive Web App**
   - Real-time Î² slider
   - Visual + audio in sync
   - Export personalized "emergence soundscapes"

3. **Live Performance**
   - Modular synthesizer implementation
   - Real-time UTAC parameter control
   - Improvisation with field type morphing

4. **Collaboration Opportunities**
   - Composers: Create UTAC-based compositions
   - Sound artists: Field recordings + UTAC synthesis
   - Data viz artists: Audiovisual installations

---

## ðŸ“Š Validation

**Perceptual tests:**
- âœ“ Î² order preserved: Higher Î² â†’ perceivably higher pitch
- âœ“ Field types distinguishable: Blind tests show >80% accuracy
- âœ“ Threshold crossings audible: Peak amplitude clearly marks Î˜

**Scientific accuracy:**
- âœ“ Ïƒ(Î²(R-Î˜)) mapped faithfully to amplitude envelope
- âœ“ Î²-to-frequency scaling preserves relative differences
- âœ“ Metadata tracks all parameters for reproducibility

---

## ðŸŽ¼ Dynamic Threshold Choir (NEW!)

> *"When the AMOC destabilizes, its voice begins to tremble. The choir sings the warning."*

**Multi-voice real-time sonification** where multiple systems "sing" simultaneously, each with its own Î²-character. Spatial positioning and destabilization effects create a rich, evolving soundscape.

### Features

- **Multi-Voice Synthesis:** Multiple systems (AMOC, LLM, ecosystems) sing together
- **Spatial Audio:** Stereo panning positions each system in space
- **Destabilization Effects:**
  - **Tremolo** (amplitude modulation) â†’ Sound trembles
  - **Vibrato** (frequency modulation) â†’ Pitch wavers
  - **Noise Injection** â†’ Chaos increases
- **Real-time Updates:** Voices respond to live data changes
- **Event Logging:** Track destabilization events with timestamps

### Quick Start

```bash
# Run demo with simulated data (AMOC, LLM, Ecosystem)
python -m sonification.dynamic_threshold_choir --demo --duration 30

# Output: 3 voices evolving over time, spatial stereo mix
```

### Python API

```python
from sonification import ThresholdChoir

# Create choir
choir = ThresholdChoir(sample_rate=44100)

# Add voices with spatial positioning
choir.add_voice("AMOC", beta=4.2, theta=50.0, pan=-0.6)     # Left
choir.add_voice("LLM", beta=3.47, theta=100.0, pan=0.0)     # Center
choir.add_voice("Ecosystem", beta=2.8, theta=500.0, pan=0.6) # Right

# Update with live data
from datetime import datetime
choir.update_voice("AMOC", new_R=45.0, timestamp=datetime.now())

# Render stereo audio
audio = choir.render(duration=10.0)  # Shape: (2, sample_rate * duration)

# Save
choir.save_wav("output/choir.wav", duration=10.0)
```

### Destabilization Dynamics

When a voice approaches its threshold (R â†’ Î˜):
- **Stability metric** decreases: `stability = 1 / (1 + distance + rate_of_change)`
- **Tremolo** activates: Sound begins to tremble (3-13 Hz modulation)
- **Vibrato** increases: Pitch starts to waver
- **Events logged**: Destabilization events stored with timestamps

When stability < 0.3:
- **Extreme effects**: Noise injection, harmonic distortion
- **Visual metaphor**: The system is "crying out" before tipping

### Demo Scenarios

Run `python sonification/examples/choir_demo.py` for 4 demos:

1. **Basic Choir** - Three voices in stable state
2. **Destabilization** - AMOC collapses, voice trembles
3. **Full Evolution** - All systems evolve over 15s
4. **Spatial Positioning** - 5 voices across stereo field

### Architecture

```
ThresholdChoir
â”œâ”€ VoiceState (per system)
â”‚  â”œâ”€ beta, theta, current_R
â”‚  â”œâ”€ stability (computed from distance + rate)
â”‚  â””â”€ pan (stereo position)
â”œâ”€ DestabilizationEffects
â”‚  â”œâ”€ tremolo(signal, rate, depth)
â”‚  â”œâ”€ vibrato(freq, rate, depth)
â”‚  â”œâ”€ noise_injection(signal, level)
â”‚  â””â”€ harmonic_distortion(signal, amount)
â””â”€ Spatial mixer (equal-power panning)
```

### Data Sources

**Currently implemented:**
- Simulators for AMOC, LLM scaling, ecosystem collapse

**Future (planned):**
- NOAA real-time climate data
- LLM API telemetry (OpenAI, Anthropic)
- Generic sensor feeds (MQTT, WebSocket)

### Use Cases

- **Climate Monitoring:** Sonify AMOC strength in real-time
- **AI Safety:** Hear LLM capability emergence during training
- **Installations:** Multi-channel spatial audio (5.1, Dolby Atmos)
- **Research:** Auditory pattern recognition in multi-system dynamics
- **Education:** Interactive exhibits where visitors "conduct" the choir

### Example Output

```
ðŸŽµ Dynamic Threshold Choir
   Voices: AMOC, LLM_GPT, Ecosystem

   AMOC        : Î²=4.20, Î˜=50.0, R=45.0, stability=0.65 (trembling)
   LLM_GPT     : Î²=3.47, Î˜=100.0, R=105.0, stability=0.45 (post-emergence)
   Ecosystem   : Î²=2.80, Î˜=500.0, R=300.0, stability=0.18 (collapsing!)

   Destabilization events: 3
   - Ecosystem: stability=0.18 (critical!)
   - AMOC: stability=0.28 (unstable)
```

---

## ðŸ”® Future Extensions

- [x] **Real-time sonification** from live data streams âœ… *Dynamic Threshold Choir*
- [x] **Spatial audio (stereo)** for multi-field compositions âœ… *Dynamic Threshold Choir*
- [ ] **5.1/Atmos spatial audio** for immersive installations
- [ ] **MIDI export** for DAW integration
- [ ] **Integration with simulator** for audiovisual presets
- [ ] **Machine learning** to learn optimal acoustic mappings
- [ ] **WebSocket streaming** for real-time web apps
- [ ] **NOAA/API connectors** for live climate/LLM data
- [ ] **Community presets** - submit your own field mappings!

---

## ðŸ“š References

### Sonification Research
- Hermann, T. (2008). *Taxonomy and definitions for sonification and auditory display.*
- Ballora, M. (2014). *Sonification strategies for the multi-level structure of proteins.*
- Vogt, K. (2010). *Sonification of simulations in computational physics.*

### UTAC Theory
- RÃ¶mer, J. B. (2025). *Universal Threshold Field Model (UTAC).*
- Wei et al. (2022). *Emergent abilities of large language models.*
- Field Type Classification v1.1 (Î·Â²=0.68)

---

## ðŸ¤ Contributing

Ideas for new presets, acoustic profiles, or field type mappings?

1. Fork the repo
2. Add your preset to `sonification/presets/`
3. Test with `python -m sonification.utac_sonification --preset your_preset`
4. Submit PR with audio examples + metadata

---

## ðŸ“œ License

MIT License - See LICENSE file

---

## ðŸ’¬ Contact

- **GitHub Issues:** https://github.com/GenesisAeon/Feldtheorie/issues
- **Zenodo DOI:** 10.5281/zenodo.17520987
- **Author:** Johann B. RÃ¶mer

---

**"Listen to emergence. Hear the threshold. Feel the criticality."** ðŸŒŠâœ¨
