run_id,model_family,architecture,n_params_millions,dataset,training_step,loss,capability_score,R_proxy,Theta_proxy,beta_estimate,beta_uncertainty,field_type,grokking_detected,notes
llm_001,GPT,transformer,125,open_web,0,4.5,0.05,0.02,0.85,1.2,0.3,Type-3,False,Initial random weights
llm_001,GPT,transformer,125,open_web,1000,3.2,0.15,0.12,0.85,1.8,0.3,Type-3,False,Early learning
llm_001,GPT,transformer,125,open_web,5000,2.1,0.35,0.35,0.82,2.4,0.4,Type-4,False,Pattern recognition emerges
llm_001,GPT,transformer,125,open_web,10000,1.5,0.52,0.52,0.80,3.1,0.5,Type-4,False,Syntax mastery
llm_001,GPT,transformer,125,open_web,20000,1.2,0.68,0.68,0.78,3.6,0.5,Type-5,False,Pre-grokking plateau
llm_001,GPT,transformer,125,open_web,22000,0.95,0.72,0.75,0.76,3.9,0.6,Type-5,True,Grokking onset (R≈Θ)
llm_001,GPT,transformer,125,open_web,23000,0.65,0.85,0.92,0.75,5.8,0.8,Type-5,True,Grokking peak (cubic jump!)
llm_001,GPT,transformer,125,open_web,25000,0.55,0.88,0.95,0.74,4.5,0.6,Type-5,True,Post-grokking relaxation
llm_001,GPT,transformer,125,open_web,30000,0.48,0.91,0.98,0.72,4.3,0.5,Type-5,False,Convergence to Φ³
llm_001,GPT,transformer,125,open_web,40000,0.45,0.93,1.00,0.70,4.2,0.4,Type-5,False,Stable at fixpoint
llm_002,GPT,transformer,350,open_web,0,4.8,0.03,0.01,0.88,1.1,0.3,Type-3,False,Initial random weights
llm_002,GPT,transformer,350,open_web,1000,3.5,0.12,0.10,0.88,1.6,0.3,Type-3,False,Early learning
llm_002,GPT,transformer,350,open_web,5000,2.3,0.32,0.32,0.85,2.2,0.4,Type-4,False,Pattern recognition
llm_002,GPT,transformer,350,open_web,10000,1.6,0.48,0.48,0.83,2.9,0.4,Type-4,False,Syntax mastery
llm_002,GPT,transformer,350,open_web,20000,1.1,0.65,0.65,0.80,3.5,0.5,Type-5,False,Pre-grokking
llm_002,GPT,transformer,350,open_web,22500,0.88,0.70,0.78,0.78,4.0,0.6,Type-5,True,Grokking onset
llm_002,GPT,transformer,350,open_web,23500,0.62,0.82,0.95,0.77,6.2,0.9,Type-5,True,Grokking peak (cubic jump!)
llm_002,GPT,transformer,350,open_web,25000,0.52,0.87,0.97,0.75,4.6,0.6,Type-5,True,Post-grokking
llm_002,GPT,transformer,350,open_web,30000,0.46,0.90,0.99,0.73,4.4,0.5,Type-5,False,Convergence
llm_002,GPT,transformer,350,open_web,40000,0.43,0.92,1.01,0.71,4.25,0.4,Type-5,False,Stable at Φ³
llm_003,GPT,transformer,1300,open_web,0,5.1,0.02,0.01,0.90,1.0,0.3,Type-3,False,Initial (GPT-2 scale)
llm_003,GPT,transformer,1300,open_web,1000,3.8,0.10,0.08,0.90,1.5,0.3,Type-3,False,Early learning
llm_003,GPT,transformer,1300,open_web,5000,2.5,0.28,0.28,0.88,2.1,0.4,Type-4,False,Pattern recognition
llm_003,GPT,transformer,1300,open_web,10000,1.7,0.45,0.45,0.85,2.8,0.4,Type-4,False,Syntax mastery
llm_003,GPT,transformer,1300,open_web,20000,1.0,0.62,0.62,0.82,3.4,0.5,Type-5,False,Pre-grokking
llm_003,GPT,transformer,1300,open_web,23000,0.82,0.68,0.80,0.80,4.1,0.6,Type-5,True,Grokking onset
llm_003,GPT,transformer,1300,open_web,24000,0.58,0.80,0.96,0.78,6.5,1.0,Type-5,True,Grokking peak (extreme jump!)
llm_003,GPT,transformer,1300,open_web,26000,0.49,0.85,0.98,0.76,4.7,0.6,Type-5,True,Post-grokking
llm_003,GPT,transformer,1300,open_web,30000,0.44,0.89,1.00,0.74,4.5,0.5,Type-5,False,Convergence
llm_003,GPT,transformer,1300,open_web,40000,0.41,0.91,1.02,0.72,4.28,0.4,Type-5,False,Stable at Φ³
llm_004,LLaMA,transformer,7000,mixed,0,5.3,0.01,0.00,0.92,0.9,0.3,Type-3,False,Initial (LLaMA-7B)
llm_004,LLaMA,transformer,7000,mixed,1000,4.0,0.08,0.06,0.92,1.4,0.3,Type-3,False,Early learning
llm_004,LLaMA,transformer,7000,mixed,5000,2.7,0.25,0.25,0.90,2.0,0.4,Type-4,False,Pattern recognition
llm_004,LLaMA,transformer,7000,mixed,10000,1.8,0.42,0.42,0.88,2.7,0.4,Type-4,False,Syntax mastery
llm_004,LLaMA,transformer,7000,mixed,20000,0.95,0.60,0.60,0.85,3.3,0.5,Type-5,False,Pre-grokking
llm_004,LLaMA,transformer,7000,mixed,24000,0.78,0.67,0.82,0.82,4.2,0.6,Type-5,True,Grokking onset
llm_004,LLaMA,transformer,7000,mixed,25000,0.56,0.78,0.97,0.80,6.8,1.1,Type-5,True,Grokking peak (large model jump!)
llm_004,LLaMA,transformer,7000,mixed,27000,0.47,0.83,0.99,0.78,4.8,0.6,Type-5,True,Post-grokking
llm_004,LLaMA,transformer,7000,mixed,30000,0.43,0.87,1.01,0.76,4.6,0.5,Type-5,False,Convergence
llm_004,LLaMA,transformer,7000,mixed,40000,0.40,0.90,1.03,0.74,4.32,0.4,Type-5,False,Stable at Φ³
llm_005,Claude,transformer,52000,rlhf,0,5.5,0.01,0.00,0.95,0.8,0.3,Type-3,False,Initial (Claude-2 scale)
llm_005,Claude,transformer,52000,rlhf,1000,4.2,0.06,0.04,0.95,1.3,0.3,Type-3,False,Early RLHF
llm_005,Claude,transformer,52000,rlhf,5000,2.9,0.22,0.22,0.92,1.9,0.4,Type-4,False,Pattern recognition
llm_005,Claude,transformer,52000,rlhf,10000,1.9,0.39,0.39,0.90,2.6,0.4,Type-4,False,Instruction following
llm_005,Claude,transformer,52000,rlhf,20000,0.92,0.58,0.58,0.87,3.2,0.5,Type-5,False,Pre-grokking
llm_005,Claude,transformer,52000,rlhf,25000,0.75,0.65,0.84,0.84,4.3,0.6,Type-5,True,Grokking onset
llm_005,Claude,transformer,52000,rlhf,26000,0.54,0.76,0.98,0.82,7.2,1.2,Type-5,True,Massive grokking (largest model!)
llm_005,Claude,transformer,52000,rlhf,28000,0.46,0.81,1.00,0.80,4.9,0.6,Type-5,True,Post-grokking
llm_005,Claude,transformer,52000,rlhf,30000,0.42,0.85,1.02,0.78,4.7,0.5,Type-5,False,Convergence
llm_005,Claude,transformer,52000,rlhf,40000,0.39,0.88,1.04,0.76,4.35,0.4,Type-5,False,Stable at Φ³
llm_006,Mistral,transformer,7300,mixed,0,5.2,0.02,0.01,0.92,0.95,0.3,Type-3,False,Initial (Mistral-7B)
llm_006,Mistral,transformer,7300,mixed,1000,3.9,0.09,0.07,0.92,1.45,0.3,Type-3,False,Early learning
llm_006,Mistral,transformer,7300,mixed,5000,2.6,0.26,0.26,0.90,2.05,0.4,Type-4,False,Pattern recognition
llm_006,Mistral,transformer,7300,mixed,10000,1.75,0.43,0.43,0.87,2.75,0.4,Type-4,False,Syntax mastery
llm_006,Mistral,transformer,7300,mixed,20000,0.93,0.61,0.61,0.84,3.35,0.5,Type-5,False,Pre-grokking
llm_006,Mistral,transformer,7300,mixed,24500,0.76,0.68,0.83,0.81,4.25,0.6,Type-5,True,Grokking onset
llm_006,Mistral,transformer,7300,mixed,25500,0.55,0.79,0.98,0.79,6.9,1.1,Type-5,True,Grokking peak
llm_006,Mistral,transformer,7300,mixed,27500,0.465,0.84,1.00,0.77,4.85,0.6,Type-5,True,Post-grokking
llm_006,Mistral,transformer,7300,mixed,30000,0.425,0.88,1.02,0.75,4.65,0.5,Type-5,False,Convergence
llm_006,Mistral,transformer,7300,mixed,40000,0.395,0.905,1.04,0.73,4.33,0.4,Type-5,False,Stable at Φ³
