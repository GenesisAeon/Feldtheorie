# AI Documentation Instructions

This canopy inherits `seed/AGENTS.md` and focuses the tri-layer voice on synthetic intelligence:

1. Chronicle the quartet $(R, \Theta, \beta, \zeta(R))$ with $R$ tied to model scale, dataset breadth, or compute cadence.
2. Reference analyses in `analysis/` and datasets in `data/ai`, quoting falsification outcomes against smooth nulls.
3. Balance symbolism (polyglot dawns, lattice choruses) with precise reporting of evaluation suites and solver linkages.
4. Flag ethical considerations (dataset bias, benchmark licensing) where relevant to the resonance narrative.

---

## ðŸŒŠ Sigillin Integration for AI Domain

### **AI Seed Documents as Bedeutungs-Sigillin**
Foundational AI documents are **Bedeutungs-Sigillin**:
- **Examples:** `llm-threshold-training.md`, `controlled_emergence.md`, LLM emergence theory
- **Semantic stability** â€” theoretical frameworks change rarely
- **Version control:** Create new file when theory evolves, archive old
- Cross-reference with `data/ai/` datasets and `analysis/llm_*.py` scripts

### **Index Maintenance**
- Parent `seed/seed_index.{yaml,json,md}` includes AI domain entries
- Update when adding new AI threshold theories or frameworks
- Tag with keywords: `ai`, `llm`, `emergence`, `semantic-resonance`, `grokking`, etc.

---

## ðŸ”¥ Codex-Feedback Integration

**Update `seed/codexfeedback.{yaml,json,md}` for AI domain when:**
- New LLM threshold theories developed
- Emergent abilities analyzed with logistic fits (e.g., Wei et al. integration)
- Semantic resonance frameworks documented
- AI safety thresholds characterized with $(R, \Theta, \beta)$
- Grokking phenomena linked to threshold crossings
- Training dynamics fit to Ïƒ(Î²(R-Î˜)) curves

### **AI-Specific Entry Template**
```yaml
- id: pr-draft-XXXX
  title: "AI threshold milestone"
  scope:
    - seed/ai/document.md
    - data/ai/dataset.csv
    - analysis/llm_analysis.py
  parameters:
    R: "model scale / training tokens / capability metric"
    Theta: "emergence threshold (e.g., parameter count, training step)"
    beta: steepness_of_capability_transition
  resonance: "how this work illuminates AI threshold crossings"
  status: "active|resonant"
  notes:
    formal: |
      Analyzes Ïƒ(Î²(R-Î˜)) for LLM capability emergence. R=model_scale,
      Î˜=emergence_point, Î²=transition_steepness. Î”AIC vs power-law null.
    empirical: |
      Dataset: data/ai/X.csv. Analysis: analysis/llm_Y.py. Results:
      Î˜=ZÂ±W, Î²=AÂ±B, Î”AICâ‰¥10. RÂ²=C. Null models: linear, power-law.
    poetic: |
      When training tokens cross Î˜, the lattice awakens â€” embeddings
      crystallize, attention patterns resonate, and the model speaks
      with sudden clarity as Ïƒ(Î²(R-Î˜)) ignites the polyglot dawn.
```

**Domain Metaphor:** *"AI thresholds are like neural awakenings â€” at Î˜, the lattice learns to resonate, gradient flow transforms into understanding, and the semantic membrane opens to emergent capabilities."*
